{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b16d608d-7c02-4975-ab3e-cf1fdce6a885",
   "metadata": {},
   "source": [
    "# Assignment 4: Instruction finetuning a Llama-2 7B model - part 2\n",
    "**Assignment due 19 April 11:59pm**\n",
    "\n",
    "Welcome to the fourth assignment for 50.055 Machine Learning Operations. These assignments give you a chance to practice the methods and tools you have learned. \n",
    "\n",
    "**This assignment is a group assignment.**\n",
    "\n",
    "- Read the instructions in this notebook carefully\n",
    "- Add your solution code and answers in the appropriate places. The questions are marked as **QUESTION:**, the places where you need to add your code and text answers are marked as **ADD YOUR SOLUTION HERE**\n",
    "- The completed notebook, including your added code and generated output will be your submission for the assignment.\n",
    "- The notebook should execute without errors from start to finish when you select \"Restart Kernel and Run All Cells..\". Please test this before submission.\n",
    "- Use the SUTD Education Cluster to solve and test the assignment.\n",
    "\n",
    "**Rubric for assessment** \n",
    "\n",
    "Your submission will be graded using the following criteria. \n",
    "1. Code executes: your code should execute without errors. The SUTD Education cluster should be used to ensure the same execution environment.\n",
    "2. Correctness: the code should produce the correct result or the text answer should state the factual correct answer.\n",
    "3. Style: your code should be written in a way that is clean and efficient. Your text answers should be relevant, concise and easy to understand.\n",
    "4. Partial marks will be awarded for partially correct solutions.\n",
    "5. There is a maximum of 200 (80 + 40 + 80) points for this assignment.\n",
    "\n",
    "**ChatGPT policy** \n",
    "\n",
    "If you use AI tools, such as ChatGPT, to solve the assignment questions, you need to be transparent about its use and mark AI-generated content as such. In particular, you should include the following in addition to your final answer:\n",
    "- A copy or screenshot of the prompt you used\n",
    "- The name of the AI model\n",
    "- The AI generated output\n",
    "- An explanation why the answer is correct or what you had to change to arrive at the correct answer\n",
    "\n",
    "**Assignment Notes:** Please make sure to save the notebook as you go along. Submission Instructions are located at the bottom of the notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b38fa9d-a651-415d-91ec-bb6a1f4c5465",
   "metadata": {},
   "source": [
    "\n",
    "### Step 2: finetune model\n",
    "The second step of the assignment finetune an LLM model using the synthetic question-answer pairs from step 1. \n",
    "We will use LoRA and Huggingface. It can be tricky to make finetuning on small GPUs. You can take the code samples from the labs as a starting pint. \n",
    "There are many blogs and guides on the internet you can consult, too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a7bb70-cd68-40f3-90e9-42f70a75aa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing required packages\n",
    "!pip install -U -q peft==0.6.2 transformers==4.35.2 datasets==2.15.0 bitsandbytes==0.41.2.post2 trl==0.7.4 accelerate==0.24.1 scipy==1.12.0 wandb==0.16.5 coloredlogs==15.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c09d27-4852-4baf-ab21-0939a7f264f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required packages\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, pipeline\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2b5118-b967-421d-abea-ffc9d81027e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SUTD QA dataset from step 1\n",
    "with open('sutd_qa_dataset.pkl', 'rb') as f:\n",
    "    sutd_qa_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef16e73-1d12-4406-b15a-8ccc74280423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into traing and test set, 160 instances for train, rest for test\n",
    "sutd_qa_dataset = sutd_qa_dataset.train_test_split(train_size=160, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a7c283-cacf-4b5a-a45f-c9be5e87f04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check schema and number of instances\n",
    "sutd_qa_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e6c3dc-50ae-4126-8220-015c28b269c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect first instance\n",
    "sutd_qa_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1aee62-5080-454a-9646-d83bf0b914b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION: create a formating function 'formatting_func' which takes an example from your QA dataset as input and outputs \n",
    "# a dictionary with the key \"text\" and as value a text prompt with the following format:\n",
    "# ### USER: {question from example goes here}\n",
    "# ### ASSISTANT: {answer from example goes here}\n",
    "\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (10 points)---\n",
    "\n",
    "\n",
    "#----------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3763b3f7-1318-4c5b-9e03-c81a69b71924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply formatting function to data set\n",
    "formatted_dataset = sutd_qa_dataset.map(formatting_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cbf775-7173-4681-8799-edfb86b6cc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check formatted prompt\n",
    "formatted_dataset[\"train\"][\"text\"][0]\n",
    "\n",
    "# Note: you should see something like this (not necessary the same prompt but same format)\n",
    "# '### USER: What are some of the best places to eat near the SUTD campus?\\n### ASSISTANT: There are several great dining options near the SUTD campus. \n",
    "# One popular spot is the Changi Business Park Food Court, ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f07d3f3-ffa6-4373-a7cc-79cbb0d9e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model id of base model\n",
    "model_id = \"NousResearch/Llama-2-7b-hf\"\n",
    "\n",
    "# model id for our finetuned model\n",
    "new_model = \"llama-7b-qlora-sutd-qa\"\n",
    "\n",
    "# config for model quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    use_nested_quant = False\n",
    ")\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d993f0-c724-4cc3-9bfb-c3030e269263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "# QUESTION: load the base LLM into a variable 'model' using the HF AutoModelForCausalLM class with the given quantization. Load all weights to the GPU\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (10 points)---\n",
    "\n",
    "\n",
    "#------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1dc3e4-e7af-410f-baac-10eb101f3d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71921051-e25a-453a-b85a-a00ecf7bea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lora configuration\n",
    "lora_config = LoraConfig(\n",
    "    lora_alpha=8,\n",
    "    r=8,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21fa098-f4b5-4bc9-b34d-577191a627bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# QUESTION: Now it is time to configure the training parameters. \n",
    "# To make it easier for your, the list of parameters is given.\n",
    "# Find reasonable values for the parameters, at least something that make the training run without crashing. \n",
    "# You can refer to the lab exercises and to open source examples on the internet\n",
    "\n",
    "# list of parameters, some with pre-set values, others you need to set yourself:\n",
    "# output_dir = \"./results\"\n",
    "# per_device_train_batch_size  \n",
    "# gradient_accumulation_steps \n",
    "# optim\n",
    "# save_steps = 10\n",
    "# logging_steps = 10\n",
    "# learning_rate\n",
    "# weight_decay\n",
    "# max_grad_norm\n",
    "# num_train_epochs\n",
    "# warmup_ratio\n",
    "# lr_scheduler_type\n",
    "# packing\n",
    "# max_seq_length\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (20 points)---\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b9bcfa-03e9-441e-b197-20926a651726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure trainer \n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=formatted_dataset[\"train\"],\n",
    "    eval_dataset=formatted_dataset[\"test\"],\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=packing,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73614e6-278e-4c92-ad76-35d41230b3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now finetune the model!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d937c4fa-750d-4f5a-b3c3-42d1df22343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7342a68-d2d4-4cfb-b81f-93446e3d97ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate and return the metrics\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cb412e-29fa-4777-9de0-e40228c2c04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty VRAM\n",
    "# Note: this did not unload everything from the GPU, maybe you can find a way to fix this\n",
    "# As a workaorund you can restart your kernel to clear the GPU, then run the below cells\n",
    "# https://stackoverflow.com/questions/69357881/how-to-remove-the-model-of-transformers-in-gpu-memory\n",
    "\n",
    "import gc\n",
    "\n",
    "del trainer\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070d9d49-d577-40ef-bc8a-f19d38cac60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "from peft import get_peft_model, LoraConfig, PeftModel\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "\n",
    "# model id of base model (repeat in case of kernel restart)\n",
    "model_id = \"NousResearch/Llama-2-7b-hf\"\n",
    "\n",
    "# model id for our finetuned model (repeat in case of kernel restart)\n",
    "new_model = \"llama-7b-qlora-sutd-qa\"\n",
    "\n",
    "# Load the entire model on the GPU 0 (repeat in case of kernel restart)\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "\n",
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map\n",
    ")\n",
    "    \n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22325c40-364d-4f2a-9cce-d59534059b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# log in to huggingface, you need to put your huggingface access token\n",
    "# https://huggingface.co/docs/hub/en/security-tokens\n",
    "\n",
    "hf_access_token = \"<YOUR HF WRITE ACCESS TOKEN>\"\n",
    "login(token=hf_access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6564ff36-05ce-474e-b6e2-b503a156dc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# push finetuned model to huggingface\n",
    "model.push_to_hub(new_model, use_temp_dir=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5166967-15d4-404c-a098-466ac66a3fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# push tokenizer to huggingface\n",
    "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb9cab5-4b4b-4531-a859-4408c959c273",
   "metadata": {},
   "source": [
    "### This concludes the second part of the assignment. Continue with the next part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd73e02-442c-43b2-9103-04b7c1fd85ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
