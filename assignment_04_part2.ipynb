{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b16d608d-7c02-4975-ab3e-cf1fdce6a885",
   "metadata": {},
   "source": [
    "# Assignment 4: Instruction finetuning a Llama-2 7B model - part 2\n",
    "**Assignment due 19 April 11:59pm**\n",
    "\n",
    "Welcome to the fourth assignment for 50.055 Machine Learning Operations. These assignments give you a chance to practice the methods and tools you have learned. \n",
    "\n",
    "**This assignment is a group assignment.**\n",
    "\n",
    "- Read the instructions in this notebook carefully\n",
    "- Add your solution code and answers in the appropriate places. The questions are marked as **QUESTION:**, the places where you need to add your code and text answers are marked as **ADD YOUR SOLUTION HERE**\n",
    "- The completed notebook, including your added code and generated output will be your submission for the assignment.\n",
    "- The notebook should execute without errors from start to finish when you select \"Restart Kernel and Run All Cells..\". Please test this before submission.\n",
    "- Use the SUTD Education Cluster to solve and test the assignment.\n",
    "\n",
    "**Rubric for assessment** \n",
    "\n",
    "Your submission will be graded using the following criteria. \n",
    "1. Code executes: your code should execute without errors. The SUTD Education cluster should be used to ensure the same execution environment.\n",
    "2. Correctness: the code should produce the correct result or the text answer should state the factual correct answer.\n",
    "3. Style: your code should be written in a way that is clean and efficient. Your text answers should be relevant, concise and easy to understand.\n",
    "4. Partial marks will be awarded for partially correct solutions.\n",
    "5. There is a maximum of 200 (80 + 40 + 80) points for this assignment.\n",
    "\n",
    "**ChatGPT policy** \n",
    "\n",
    "If you use AI tools, such as ChatGPT, to solve the assignment questions, you need to be transparent about its use and mark AI-generated content as such. In particular, you should include the following in addition to your final answer:\n",
    "- A copy or screenshot of the prompt you used\n",
    "- The name of the AI model\n",
    "- The AI generated output\n",
    "- An explanation why the answer is correct or what you had to change to arrive at the correct answer\n",
    "\n",
    "**Assignment Notes:** Please make sure to save the notebook as you go along. Submission Instructions are located at the bottom of the notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b38fa9d-a651-415d-91ec-bb6a1f4c5465",
   "metadata": {},
   "source": [
    "\n",
    "### Step 2: finetune model\n",
    "The second step of the assignment finetune an LLM model using the synthetic question-answer pairs from step 1. \n",
    "We will use LoRA and Huggingface. It can be tricky to make finetuning on small GPUs. You can take the code samples from the labs as a starting pint. \n",
    "There are many blogs and guides on the internet you can consult, too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40a7bb70-cd68-40f3-90e9-42f70a75aa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing required packages\n",
    "!pip install -U -q peft==0.6.2 transformers==4.35.2 datasets==2.15.0 bitsandbytes==0.41.2.post2 trl==0.7.4 accelerate==0.24.1 scipy==1.12.0 wandb==0.16.5 coloredlogs==15.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29c09d27-4852-4baf-ab21-0939a7f264f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load required packages\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, pipeline\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb2b5118-b967-421d-abea-ffc9d81027e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SUTD QA dataset from step 1\n",
    "with open('sutd_qa_dataset.pkl', 'rb') as f:\n",
    "    sutd_qa_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ef16e73-1d12-4406-b15a-8ccc74280423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into traing and test set, 160 instances for train, rest for test\n",
    "sutd_qa_dataset = sutd_qa_dataset.train_test_split(train_size=160, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67a7c283-cacf-4b5a-a45f-c9be5e87f04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 160\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 40\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check schema and number of instances\n",
    "sutd_qa_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45e6c3dc-50ae-4126-8220-015c28b269c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are the core academic programs offered at SUTD?',\n",
       " 'answer': 'The Singapore University of Technology and Design (SUTD) offers a range of academic programs with a focus on technology and design. Core programs include Master of Architecture, Master of Engineering, Master of Innovation by Design, and Master of Science in various fields. These programs provide a strong foundation for students seeking to excel in these areas.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect first instance\n",
    "sutd_qa_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b1aee62-5080-454a-9646-d83bf0b914b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION: create a formating function 'formatting_func' which takes an example from your QA dataset as input and outputs \n",
    "# a dictionary with the key \"text\" and as value a text prompt with the following format:\n",
    "# ### USER: {question from example goes here}\n",
    "# ### ASSISTANT: {answer from example goes here}\n",
    "\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (10 points)---\n",
    "def formatting_func(example):\n",
    "    formatted_text = f\"### USER: {example['question']}\\n### ASSISTANT: {example['answer']}\"\n",
    "    return {\"text\": formatted_text}\n",
    "#----------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3763b3f7-1318-4c5b-9e03-c81a69b71924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b80ceb8abaf46578598ece6670e8107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/160 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e241acf1edf40fca48ed485cfb48879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# apply formatting function to data set\n",
    "formatted_dataset = sutd_qa_dataset.map(formatting_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7cbf775-7173-4681-8799-edfb86b6cc52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### USER: What are the core academic programs offered at SUTD?\\n### ASSISTANT: The Singapore University of Technology and Design (SUTD) offers a range of academic programs with a focus on technology and design. Core programs include Master of Architecture, Master of Engineering, Master of Innovation by Design, and Master of Science in various fields. These programs provide a strong foundation for students seeking to excel in these areas.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check formatted prompt\n",
    "formatted_dataset[\"train\"][\"text\"][0]\n",
    "\n",
    "# Note: you should see something like this (not necessary the same prompt but same format)\n",
    "# '### USER: What are some of the best places to eat near the SUTD campus?\\n### ASSISTANT: There are several great dining options near the SUTD campus. \n",
    "# One popular spot is the Changi Business Park Food Court, ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f07d3f3-ffa6-4373-a7cc-79cbb0d9e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model id of base model\n",
    "model_id = \"NousResearch/Llama-2-7b-hf\"\n",
    "\n",
    "# model id for our finetuned model\n",
    "new_model = \"llama-7b-qlora-sutd-qa\"\n",
    "\n",
    "# config for model quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    use_nested_quant = False\n",
    ")\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3d993f0-c724-4cc3-9bfb-c3030e269263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a75f5dc51b464fb4448155f3a3e171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "\n",
    "# QUESTION: load the base LLM into a variable 'model' using the HF AutoModelForCausalLM class with the given quantization. Load all weights to the GPU\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (10 points)---\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "#------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc1dc3e4-e7af-410f-baac-10eb101f3d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71921051-e25a-453a-b85a-a00ecf7bea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lora configuration\n",
    "lora_config = LoraConfig(\n",
    "    lora_alpha=8,\n",
    "    r=8,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d21fa098-f4b5-4bc9-b34d-577191a627bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# QUESTION: Now it is time to configure the training parameters. \n",
    "# To make it easier for your, the list of parameters is given.\n",
    "# Find reasonable values for the parameters, at least something that make the training run without crashing. \n",
    "# You can refer to the lab exercises and to open source examples on the internet\n",
    "\n",
    "# list of parameters, some with pre-set values, others you need to set yourself:\n",
    "# output_dir = \"./results\"\n",
    "# per_device_train_batch_size  \n",
    "# gradient_accumulation_steps \n",
    "# optim\n",
    "# save_steps = 10\n",
    "# logging_steps = 10\n",
    "# learning_rate\n",
    "# weight_decay\n",
    "# max_grad_norm\n",
    "# num_train_epochs\n",
    "# warmup_ratio\n",
    "# lr_scheduler_type\n",
    "\n",
    "# Arguments in SFTTrainer Class\n",
    "# packing\n",
    "# max_seq_length\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (20 points)---\n",
    "training_arguments = TrainingArguments(\n",
    "    report_to=\"none\",\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"adamw_bnb_8bit\",\n",
    "    save_steps=10,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    max_grad_norm=0.3,\n",
    "    num_train_epochs=1,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant_with_warmup\",\n",
    "    # group_by_length=True,\n",
    "    # max_steps=-1,\n",
    "    # fp16=False,\n",
    "    # bf16=False,\n",
    ")\n",
    "\n",
    "packing=False\n",
    "# print(tokenizer.model_max_length)\n",
    "max_seq_length = 512 # min(tokenizer.model_max_length, 1024) # Try 512. Can increase to 1024 or decrease to 256.\n",
    "\n",
    "#------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43b9bcfa-03e9-441e-b197-20926a651726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc4f9abdc13446e87dcafb059b5d43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/160 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "735ede856a9e4032b5d607e763702237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# configure trainer \n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=formatted_dataset[\"train\"],\n",
    "    eval_dataset=formatted_dataset[\"test\"],\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=packing,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f73614e6-278e-4c92-ad76-35d41230b3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [80/80 01:21, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.059800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.722900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.431200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.340700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.213400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.170800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.149800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.229900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=80, training_loss=1.414814281463623, metrics={'train_runtime': 82.7641, 'train_samples_per_second': 1.933, 'train_steps_per_second': 0.967, 'total_flos': 782356505149440.0, 'train_loss': 1.414814281463623, 'epoch': 1.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now finetune the model!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d937c4fa-750d-4f5a-b3c3-42d1df22343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7342a68-d2d4-4cfb-b81f-93446e3d97ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.1463383436203003,\n",
       " 'eval_runtime': 6.6184,\n",
       " 'eval_samples_per_second': 6.044,\n",
       " 'eval_steps_per_second': 0.755,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluate and return the metrics\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62cb412e-29fa-4777-9de0-e40228c2c04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty VRAM\n",
    "# Note: this did not unload everything from the GPU, maybe you can find a way to fix this\n",
    "# As a workaorund you can restart your kernel to clear the GPU, then run the below cells\n",
    "# https://stackoverflow.com/questions/69357881/how-to-remove-the-model-of-transformers-in-gpu-memory\n",
    "\n",
    "import gc\n",
    "\n",
    "del trainer\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "070d9d49-d577-40ef-bc8a-f19d38cac60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f027d08f75fc4100ad36a2004f18ee98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "from peft import get_peft_model, LoraConfig, PeftModel\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "\n",
    "# model id of base model (repeat in case of kernel restart)\n",
    "model_id = \"NousResearch/Llama-2-7b-hf\"\n",
    "\n",
    "# model id for our finetuned model (repeat in case of kernel restart)\n",
    "new_model = \"llama-7b-qlora-sutd-qa\"\n",
    "\n",
    "# Load the entire model on the GPU 0 (repeat in case of kernel restart)\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "\n",
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map\n",
    ")\n",
    "    \n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22325c40-364d-4f2a-9cce-d59534059b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# log in to huggingface, you need to put your huggingface access token\n",
    "# https://huggingface.co/docs/hub/en/security-tokens\n",
    "\n",
    "hf_access_token = \"<INSERT HF TOKEN>\"\n",
    "login(token=hf_access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6564ff36-05ce-474e-b6e2-b503a156dc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved at https://huggingface.co/jtz18/llama-7b-qlora-sutd-qa/\n",
    "\n",
    "# push finetuned model to huggingface\n",
    "model.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5166967-15d4-404c-a098-466ac66a3fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved at https://huggingface.co/jtz18/llama-7b-qlora-sutd-qa/\n",
    "\n",
    "# push tokenizer to huggingface\n",
    "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb9cab5-4b4b-4531-a859-4408c959c273",
   "metadata": {},
   "source": [
    "### This concludes the second part of the assignment. Continue with the next part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd73e02-442c-43b2-9103-04b7c1fd85ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
